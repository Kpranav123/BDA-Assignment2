{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWpCq/iD+I5pz+kWoLAyCu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kpranav123/BDA-Assignment2/blob/main/BDA_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Y_-2XiZDcZ",
        "outputId": "20b2294a-9a8b-4012-ad8a-f4622aa96f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (AUC): 0.8813358538122231\n"
          ]
        }
      ],
      "source": [
        "#Build a Classification Model with Spark with a dataset of your choice\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 1: Start Spark session\n",
        "spark = SparkSession.builder.appName(\"BankMarketingClassification\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data = spark.read.csv(\"/content/bank.csv\", header=True, inferSchema=True, sep=';')\n",
        "\n",
        "# Step 3: Data preprocessing\n",
        "# Convert label column to numeric\n",
        "label_indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
        "\n",
        "# Convert categorical features\n",
        "categorical_cols = [field for (field, dtype) in data.dtypes if dtype == \"string\" and field != \"y\"]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_cols]\n",
        "\n",
        "# Assemble features\n",
        "numeric_cols = [field for (field, dtype) in data.dtypes if dtype != \"string\"]\n",
        "indexed_categorical_cols = [col + \"_indexed\" for col in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=numeric_cols + indexed_categorical_cols, outputCol=\"features\")\n",
        "\n",
        "# Step 4: Define classifier\n",
        "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Step 5: Build pipeline\n",
        "pipeline = Pipeline(stages=indexers + [label_indexer, assembler, classifier])\n",
        "\n",
        "# Step 6: Split data\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Step 7: Train model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Step 8: Evaluate model\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Model Accuracy (AUC): {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Step 1: Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClusteringModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Load dataset\n",
        "df = spark.read.csv(\"/content/Clustering.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Optional: Preview data\n",
        "df.show(5)\n",
        "\n",
        "# Step 3: Assemble features into a single vector\n",
        "feature_columns = df.columns  # Assuming all columns are features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "assembled_data = assembler.transform(df)\n",
        "\n",
        "# Step 4: Build the KMeans model\n",
        "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\", predictionCol=\"cluster\") # predictionCol is set to \"cluster\"\n",
        "model = kmeans.fit(assembled_data)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "predictions = model.transform(assembled_data)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "evaluator = ClusteringEvaluator(predictionCol=\"cluster\") # Changed predictionCol to \"cluster\" to match KMeans\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(f\"Silhouette Score: {silhouette}\")\n",
        "\n",
        "# Optional: Show cluster assignments\n",
        "predictions.select(feature_columns + ['cluster']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cbP4vtjt3lb",
        "outputId": "1b244ef1-b4bb-40e6-d633-816cfa02e82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+----------------+\n",
            "|_c0|               x|               y|\n",
            "+---+----------------+----------------+\n",
            "|  1|3.36759599170382|3.53669397567831|\n",
            "|  2| 2.6678697659321|4.47991877277642|\n",
            "|  3|1.34417120978313|3.28259118956231|\n",
            "|  4|1.38941378084597|4.68322664948847|\n",
            "|  5| 1.6446438390549|4.32082237219094|\n",
            "+---+----------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Silhouette Score: 0.7444141380227035\n",
            "+---+-------------------+----------------+-------+\n",
            "|_c0|                  x|               y|cluster|\n",
            "+---+-------------------+----------------+-------+\n",
            "|  1|   3.36759599170382|3.53669397567831|      1|\n",
            "|  2|    2.6678697659321|4.47991877277642|      1|\n",
            "|  3|   1.34417120978313|3.28259118956231|      1|\n",
            "|  4|   1.38941378084597|4.68322664948847|      1|\n",
            "|  5|    1.6446438390549|4.32082237219094|      1|\n",
            "|  6|  0.776027424326743|2.65366676282803|      1|\n",
            "|  7|   3.26410141180453| 3.6927619550371|      1|\n",
            "|  8| -0.182646568697024|2.59282570702876|      1|\n",
            "|  9|   1.13821502906237|2.80029637463032|      1|\n",
            "| 10|  0.759164488599356|4.64805745225988|      1|\n",
            "| 11|   2.16452661434548|3.35508454569812|      1|\n",
            "| 12|-0.0937990215106019|3.40634508630138|      1|\n",
            "| 13|   1.74872820664091|2.31837611169281|      1|\n",
            "| 14|   2.07786721276868|5.31645786182762|      1|\n",
            "| 15|   2.45672740337584|1.82548362574936|      1|\n",
            "| 16|  0.614715124693451|3.24237142900606|      1|\n",
            "| 17|   2.81111239967797|1.92903785672695|      1|\n",
            "| 18|   2.71937715090084|2.39257598404609|      1|\n",
            "| 19|   1.91288396934586|4.53623722561707|      1|\n",
            "| 20|   3.23167098582873|3.01206023611846|      1|\n",
            "+---+-------------------+----------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "# Step 1: Start Spark session\n",
        "spark = SparkSession.builder.appName(\"MovieRecommendation\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the ratings dataset\n",
        "ratings = spark.read.csv(\"/content/ratings.csv\", header=True, inferSchema=True)\n",
        "ratings.printSchema()\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "# Keep only necessary columns\n",
        "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
        "\n",
        "# Step 4: Train-test split\n",
        "(training, test) = ratings.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 5: Build the ALS model\n",
        "als = ALS(\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",  # To handle NaN predictions\n",
        "    nonnegative=True,\n",
        "    implicitPrefs=False,\n",
        "    maxIter=10,\n",
        "    rank=10,\n",
        "    regParam=0.1\n",
        ")\n",
        "\n",
        "model = als.fit(training)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root-mean-square error = {rmse:.2f}\")\n",
        "\n",
        "# Step 7: Generate top 5 movie recommendations for each user\n",
        "userRecs = model.recommendForAllUsers(5)\n",
        "userRecs.show(5, truncate=False)\n",
        "\n",
        "# Optional: Generate top 5 user recommendations for each movie\n",
        "# movieRecs = model.recommendForAllItems(5)\n",
        "# movieRecs.show(5, truncate=False)\n",
        "\n",
        "# Step 8: Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glYUNhET6okv",
        "outputId": "960ed23d-e567-47ec-aa65-bd0b5803c11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- userId: integer (nullable = true)\n",
            " |-- movieId: integer (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            " |-- timestamp: integer (nullable = true)\n",
            "\n",
            "Root-mean-square error = 0.88\n",
            "+------+---------------------------------------------------------------------------------------------------+\n",
            "|userId|recommendations                                                                                    |\n",
            "+------+---------------------------------------------------------------------------------------------------+\n",
            "|1     |[{177593, 6.0177426}, {33649, 5.862716}, {3925, 5.835306}, {96004, 5.6480436}, {3379, 5.6480436}]  |\n",
            "|2     |[{25771, 4.886084}, {131724, 4.8327866}, {33649, 4.77594}, {171495, 4.6861277}, {184245, 4.672516}]|\n",
            "|3     |[{6835, 4.908595}, {5746, 4.908595}, {5181, 4.820066}, {4518, 4.7179623}, {7899, 4.4177356}]       |\n",
            "|4     |[{1241, 4.9914727}, {3851, 4.922374}, {123, 4.9020886}, {31878, 4.8874288}, {1733, 4.858461}]      |\n",
            "|5     |[{89904, 4.9000654}, {1212, 4.79261}, {6650, 4.790019}, {7096, 4.767788}, {177593, 4.7161913}]     |\n",
            "+------+---------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}